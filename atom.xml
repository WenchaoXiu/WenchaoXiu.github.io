<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WenchaoXiu</title>
  
  <subtitle>just try blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://WenchaoXiu.github.io/"/>
  <updated>2018-08-02T03:46:54.000Z</updated>
  <id>http://WenchaoXiu.github.io/</id>
  
  <author>
    <name>WenchaoXiu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习各类算法实践</title>
    <link href="http://WenchaoXiu.github.io/2018/07/17/ML-algrithom-usage/"/>
    <id>http://WenchaoXiu.github.io/2018/07/17/ML-algrithom-usage/</id>
    <published>2018-07-17T01:44:48.000Z</published>
    <updated>2018-08-02T03:46:54.000Z</updated>
    
    <content type="html"><![CDATA[<p class="description">机器学习算法的实践笔记</p><p><img src="./images/sklearn.jpg" alt="" style="width:100%"></p><a id="more"></a><h1 id="笔记简介"><a href="#笔记简介" class="headerlink" title="笔记简介"></a>笔记简介</h1><p>主要整理一下近期学习的机器学习算法具体的实现过程,主要使用的package有scikit-learn,xgboost</p><h1 id="sklearn各功能实现"><a href="#sklearn各功能实现" class="headerlink" title="sklearn各功能实现"></a>sklearn各功能实现</h1><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>1.对类别数据进行离散化<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为构件数学模型的时候类别特征可以取多个值，不同的值实际上代表的是不同特征属性，因此需要对类别特征进行离散化处理</span></span><br><span class="line"><span class="comment"># 1.利用pandas的get_dummies进行onehot编码</span></span><br><span class="line">newDF = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> DF.columns:</span><br><span class="line">tmp = pd.get_dummies(DF[col])</span><br><span class="line">tmp = pd.rename(columns = <span class="keyword">lambda</span> x: col+<span class="string">'_'</span>+str(x))</span><br><span class="line">newDF = pd.concat([newDF, tmp], axis=<span class="number">1</span>)</span><br><span class="line">x = newDF.values</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.利用sklearn的label_binarize进行onehot编码</span></span><br><span class="line"><span class="comment"># 除了上述的pd.get_dummies之外的另一种等价方法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> label_binarize</span><br><span class="line">DF_col = label_binarize(DF[col], classes = np.arange(len(pd.unique(DF[col]))))</span><br><span class="line"><span class="comment"># label_binarize返回的和pd.get_dummies返回的对象类似，列数与类别数相同，且只包含01值</span></span><br><span class="line"><span class="comment"># 区别在于，label_binarize返回的是ndarray数据，pd.get_dummies返回的是DataFrame数据</span></span><br><span class="line"><span class="comment"># 因此，np.array_equal(DF_col, pd.get_dummies(DF[col]).values)返回的是True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.sklearn的OneHotEncoder</span></span><br><span class="line"><span class="comment"># 这个功能略感鸡肋(也可能是打开方式不对)给个例子，就不赘述了</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">x = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">     [<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>],</span><br><span class="line">     [<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line">     [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],]</span><br><span class="line">ohe = OneHotEncoder(sparse=<span class="keyword">False</span>)</span><br><span class="line"><span class="keyword">print</span> ohe.fit_transform(x)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure></p><p>2.对响应变量进行转换，将类别数据转化为不同int型数字<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y = DF.accept</span><br><span class="line">y_int = pd.Categorical(y).codes <span class="comment">#得到从0~n-1的n类数字，分别对应不同的类别</span></span><br><span class="line"><span class="comment"># 与上述pd.get_dummies的区别在于，get_dummies返回的是DataFrame数据，且只有01两个值</span></span><br><span class="line"><span class="comment"># 但是上述方法返回的是ndarray数据，返回的值不止01，具体的根据类别数量进行确定</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">y_int = LabelEncoder().fit_transform(y_train)</span><br><span class="line"><span class="comment"># 实现了跟上面相同的功能，输入是不用类别的label，返回的是0~n-1的数字</span></span><br></pre></td></tr></table></figure></p><p>3.训练集测试集分割<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这样做的目的是方便测试算法泛化能力</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=<span class="number">0.7</span>)</span><br><span class="line"><span class="comment"># PS:其中x, y是ndarray数据类型，如果是DF数据的话需要使用DF.values进行转化</span></span><br></pre></td></tr></table></figure></p><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>1.PCA数据降维<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca_model = PCA(n_components=<span class="number">2</span>, whiten=<span class="keyword">True</span>)</span><br><span class="line">x_pca = pca_model.fit_transform(x)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'top2 方差'</span>, pca_model.explained_variance_</span><br><span class="line"><span class="keyword">print</span> <span class="string">'top2 方差所占比例'</span>, pca_model.explained_variance_ratio_</span><br><span class="line"><span class="comment"># 其中的n_component是选择将数据降维后，选取其中的n个特征（特征顺序按照特征值从高到低排序）</span></span><br><span class="line"><span class="comment"># pca_model.explained_variance_是对应的前两个特征的方差</span></span><br><span class="line"><span class="comment"># pca_model.explained_variance_ratio_是前两个特征的方差占总方差的比例</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.lmplot(x = <span class="string">'pc1'</span>, y=<span class="string">'pc2'</span>, data = pc_plot_data, hue=type, fit_reg=<span class="keyword">False</span>)</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.set_title(<span class="string">"Iris PCA 2 compotent"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 上述代码用来绘图</span></span><br></pre></td></tr></table></figure></p><p><a href="https://www.cnblogs.com/pinard/p/6243025.html" target="_blank" rel="noopener">使用教程</a></p><p>2.特征筛选<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用sklearn的SelectKBest进行特征选择</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest, chi2</span><br><span class="line">chi2_model = SelectKBest(chi2, k=<span class="number">2</span>)</span><br><span class="line">chi2_model.fit(x, y)</span><br><span class="line">selected_col = chi2_model.get_support(indices=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 之所以x和y都需要加进去是因为需要对每个特征和响应变量进行chi2检验，按照显著性排序，找到前两个特征（原特征）</span></span><br><span class="line"><span class="comment"># indices是为了返回对应的column数</span></span><br></pre></td></tr></table></figure></p><p><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/feature_selection.html" target="_blank" rel="noopener">官方文档</a> &amp; <a href="http://bluewhale.cc/2016-11-25/use-scikit-learn-for-feature-selection.html" target="_blank" rel="noopener">使用教程</a> &amp; <a href="http://d0evi1.com/sklearn/feature_selection/" target="_blank" rel="noopener">特征提取</a></p><p>3.原始特征变化<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">poly = PolynomialFeatures(degree = <span class="number">2</span>, include_bias=<span class="keyword">True</span>, interaction_only=<span class="keyword">False</span>)</span><br><span class="line">x_poly = poly.fit_transform(x)</span><br><span class="line"><span class="comment"># 对数据特征进行变换，degree是特征的最高维数，include_bias加不加一列1，interaction_only是否只看有交互的特征</span></span><br></pre></td></tr></table></figure></p><h2 id="构建有监督模型"><a href="#构建有监督模型" class="headerlink" title="构建有监督模型"></a>构建有监督模型</h2><p>1.逻辑回归模型用于分类（Logistic Regression）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklean.linear_model <span class="keyword">import</span> LogisticRegressionCV, Lasso</span><br><span class="line">model = LinearRegeressionCV(Cs = np.logspace(<span class="number">-3</span>,<span class="number">4</span>,<span class="number">8</span>), cv = <span class="number">5</span>, n_job = <span class="number">-1</span>)</span><br><span class="line">model.fit(x_train, y_train)</span><br><span class="line">y_train_hat = model.predict(x_train) <span class="comment"># 对训练集进行预测</span></span><br><span class="line">y_test_hat = model.predict(x_test) <span class="comment"># 对测试集进行预测</span></span><br><span class="line"><span class="keyword">print</span> model.C_ <span class="comment"># 获得各类别最佳超参数</span></span><br><span class="line"><span class="comment"># LinearRegeressionCV是一种自动确定正则化超参数的函数，其中Cs是一系列的正则化参数值，一般需要取不同数量级的数字</span></span><br><span class="line"><span class="comment"># cv是进行cross validation对超参数最优解进行确定，n_job=-1是用上全部线程，这里默认的正则是L2 norm</span></span><br><span class="line"><span class="comment"># PS:注意在进行多分类的时候选择的方法是one vers rest也就是一对多进行多分类</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意除了自带的cross-validation方法，还可以用更加普遍的方法GridSearchCV</span></span><br><span class="line">model = Lasso()</span><br><span class="line">GS_Lasso = GridSearchCV(model, param_grid=&#123;<span class="string">'alpha'</span>:np.logspace(<span class="number">-3</span>,<span class="number">4</span>,<span class="number">8</span>)&#125;, cv=<span class="number">5</span>)</span><br><span class="line">GS_Lasso.fit(x_train, y_train)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Lasso model最优参数: '</span>, GS_Lasso.best_params_</span><br></pre></td></tr></table></figure></p><p>2.决策树模型（DesitionTree）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对决策树的不同深度进行探究，求取对应的准确率</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">model = DecisionTreeClassifier()</span><br><span class="line">acc = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">15</span>):</span><br><span class="line">    model.set_params(max_depth=i)</span><br><span class="line">    model.fit(x_train, y_train)</span><br><span class="line">    y_test_hat = model.predict(x_test)</span><br><span class="line">    acc.append(metrics.accuracy_score(y_test, y_test_hat))</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>,<span class="number">15</span>), acc, <span class="string">'ro'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'depth'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#决策树进行回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">reg = DecisionTreeRegressor(criterion=<span class="string">'mse'</span>, max_depth=deep)</span><br><span class="line">dt = reg.fit(x, y)</span><br></pre></td></tr></table></figure></p><p>3.随机森林模型（RandomForest）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">RF_model = RandomForestRegressor()</span><br><span class="line">RF_model.fit(x_train, y_train)</span><br><span class="line">y_train_hat = RF_model.predict(y_train)</span><br><span class="line"><span class="comment"># 对于多值输出问题来说一般采用多值输出的方法进行模型构建，主要从两个角度：a.每个叶节点存储多个值 b.通过计算多个值的平均减少量作为split标准</span></span><br></pre></td></tr></table></figure></p><p><a href="http://sklearn.apachecn.org/cn/latest/modules/tree.html" target="_blank" rel="noopener">参考</a> &amp; <a href="https://www.cnblogs.com/pinard/p/6160412.html" target="_blank" rel="noopener">随机森林调参</a></p><p>4.bagging模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">DT_model = DecisionTreeRegressor(max_depth=<span class="number">9</span>)</span><br><span class="line">ridge_BG_model = BaggingRegressor(Pipeline([(<span class="string">'poly'</span>, PolynomialFeatures(degree=<span class="number">6</span>)), </span><br><span class="line">                                            (<span class="string">'ridge'</span>, linear_model.RidgeCV(alphas=np.logspace(<span class="number">-3</span>,<span class="number">4</span>,<span class="number">8</span>), cv=<span class="number">5</span>, fit_intercept=<span class="keyword">False</span>))])</span><br><span class="line">                                  , n_estimators=n_estimators, max_samples=max_samples)</span><br><span class="line">DT_BG_model = BaggingRegressor(DT_model, n_estimators=n_estimators, max_samples=max_samples)</span><br><span class="line"><span class="comment"># bagging是一种构建多个基分类器，对数据进行预测，并平均多个分类器的预测值的一种方法</span></span><br><span class="line"><span class="comment"># 输入需要有基分类器，基分类器的个数，以及对于样本选取的比例</span></span><br></pre></td></tr></table></figure></p><p>5.Adaboost模型<br><a href="http://www.cnblogs.com/pinard/p/6133937.html" target="_blank" rel="noopener">原理</a> &amp; <a href="https://www.cnblogs.com/pinard/p/6136914.html" target="_blank" rel="noopener">模型参数</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">base = DecisionTreeClassifier(n_estimators=<span class="number">100</span>, max_depth = <span class="number">3</span>, min_sample_split=<span class="number">4</span>, random_state=<span class="number">1</span>, oob_score=<span class="keyword">True</span>) </span><br><span class="line"><span class="comment"># 超过4个样本就进行样本的分割构建叶节点, 基分类器个数100，最大树深3，oob_score考虑带外分数</span></span><br><span class="line">AB_model = AdaBoostClassifier(base_estimator = base, learning_rate = <span class="number">0.1</span>)</span><br><span class="line">AB_model.fit(x_train, y_train)</span><br><span class="line">y_train_hat = AB_model.predict(x_train)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'训练集准确度: %.5.f'</span>%metrics.accuracy_score(y_train, y_train_hat)</span><br><span class="line"><span class="comment"># Adaboost是另一种集成模型，利用决策树模型作为基分类器，与上面的bagging的区别在于，bagging是可以并行的，各个模型的确定可以随机产生，最后平均化即可</span></span><br><span class="line"><span class="comment"># 但是对于Adaboost来说各个基分类器是串行产生的，每次新产生的分类器都与前面所有的分类器相关，具体是新的分类器是通过前面所有模型的残差和确定的</span></span><br><span class="line"><span class="comment"># 相当于不断强化基分类器与真实值的差别，因此Adaboost的基分类器需要更加泛化相比较Bagging的及模型</span></span><br><span class="line"><span class="comment"># 举个例子假如基分类器都是决策树，Bagging的深度更深，Adaboost的深度更浅</span></span><br><span class="line"><span class="comment"># 这里learning_rate实际上是fk(x)=fk−1(x)+ναkGk(x)中的ν(0&lt;ν&lt;1)，其中αk是根据分类器的错误率进行确定的模型权重</span></span><br><span class="line"><span class="comment"># Gk(x)是根据样本权重重新确定的基分类器，fk(x)是膜前获得的强分类器是k个基分类器的结合，理论上ν越小的话迭代下次数越多</span></span><br></pre></td></tr></table></figure></p><p>6.GBDT模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#GBDT 每个基分类器要相对弱一点因为是提升树</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">gbdt_model = GradientBoostingClassifier(learning_rate=<span class="number">0.1</span>, max_depth=<span class="number">3</span>, n_estimators=<span class="number">100</span>)</span><br><span class="line">gbdt_model.fit(x_train, y_train)</span><br><span class="line">gbdt_y_train_hat = gbdt_model.predict(x_train)</span><br><span class="line">gbdt_y_test_hat = gbdt_model.predict(x_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'训练集准确率: %.4f'</span>%metrics.accuracy_score(y_train, gbdt_y_train_hat)</span><br></pre></td></tr></table></figure></p><p>7.xgboost模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># xgboost也是一种快速效果好的集成模型</span></span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line">train_data = xgb.DMatrix(x_train, y_train)</span><br><span class="line">test_data = xgb.DMatrix(x_test, y_test)</span><br><span class="line">watch_list = [(test_data, <span class="string">'eval'</span>), (train_data, <span class="string">'train'</span>)]</span><br><span class="line">params = &#123;<span class="string">'eta'</span>:<span class="number">0.1</span>, <span class="string">'max_depth'</span>:<span class="number">6</span>, <span class="string">'objective'</span>:<span class="string">'multi:softmax'</span>, <span class="string">'num_class'</span>:<span class="number">3</span>&#125;</span><br><span class="line">xgb_model = xgb.train(params, train_data, num_boost_round=<span class="number">50</span>, evals=watch_list, evals_result=&#123;<span class="string">'eval_metric'</span>:<span class="string">'logloss'</span>&#125;) <span class="comment">#设置对应评估指标</span></span><br><span class="line">y_test_hat_xgb = xgb_model.predict(test_data) <span class="comment">#######test_data类型，xgb_model跟sklearn不同</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'xgb model: %.3f'</span>%(sum(y_test_hat_xgb==y_test)*<span class="number">1.0</span>/len(y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用XGBOOST的sklearn接口进行模型构建，这个方便点使用习惯和sklearn其他模型类似</span></span><br><span class="line">xgb_model = xgb.XGBClassifier(learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">100</span>, max_depth=<span class="number">2</span>, objective=<span class="string">'multi:softmax'</span>)</span><br><span class="line">xgb_model.fit(x_train, y_train)</span><br><span class="line">xgb_y_train_hat = xgb_model.predict(x_train)</span><br><span class="line">xgb_y_test_hat = xgb_model.predict(x_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'训练集准确率: %.4f'</span>%metrics.accuracy_score(y_train, xgb_y_train_hat)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'验证集准确率: %.4f'</span>%metrics.accuracy_score(y_test, xgb_y_test_hat)</span><br></pre></td></tr></table></figure></p><p><a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=fit#module-xgboost.sklearn" target="_blank" rel="noopener">sklearn API</a> &amp; <a href="https://blog.csdn.net/sb19931201/article/details/52557382" target="_blank" rel="noopener">参数解释</a> &amp; <a href="https://github.com/dmlc/xgboost/tree/master/demo#tutorials" target="_blank" rel="noopener">github使用教程</a> &amp; <a href="https://www.zhihu.com/question/41354392/answer/98658997" target="_blank" rel="noopener">GDBT和XGBOOSt区别</a> &amp; <a href="https://zhuanlan.zhihu.com/p/28663369" target="_blank" rel="noopener">Tatanic实例</a> &amp; <a href="https://zhuanlan.zhihu.com/p/31182879" target="_blank" rel="noopener"><strong>知乎教程</strong></a> &amp; <a href="https://zhuanlan.zhihu.com/p/26683576" target="_blank" rel="noopener"><strong>Ensemble模型介绍</strong></a></p><p>8.SVM模型<a href="https://www.cnblogs.com/pinard/p/6117515.html" target="_blank" rel="noopener">参考</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">model = svm.SVC(C=<span class="number">0.1</span>, kernel=<span class="string">'linear'</span>, decision_function_shape=<span class="string">'ovr'</span>, class_weight=&#123;<span class="number">-1</span>:<span class="number">1</span>, <span class="number">1</span>:weight[<span class="number">0</span>]&#125;)</span><br><span class="line">model.fit(x_train, y_train)</span><br><span class="line">y_train_hat = model.predit(x_train)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'decision_function:\n'</span>, clf.decision_function(x_train)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'\npredict:\n'</span>, clf.predict(x_train)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'支撑向量的数目：'</span>, clf.n_support_</span><br><span class="line"><span class="keyword">print</span> <span class="string">'支撑向量的系数：'</span>, clf.dual_coef_</span><br><span class="line"><span class="keyword">print</span> <span class="string">'支撑向量: '</span>, clf.support_ <span class="comment"># 这个是对应的支持向量的索引值</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'模型精度: '</span>, model.score(x_train, y_train)</span><br><span class="line"><span class="comment"># C是惩罚系数，当C很大的时候为了保证损失函数最小对应的惩罚因子需要很小，使得模型很严格，决策边界之间的距离很小</span></span><br><span class="line"><span class="comment"># class_weight是为了方便对不平衡数据的操作，如果类别数据数量级差的太多的话会使模型准确度下降</span></span><br><span class="line"><span class="comment"># kernel可以选择不同的核函数，decision_function_shape是进行one vers rest，可以使用ovo,这样的话一对一</span></span><br><span class="line"><span class="comment"># decision_function计算每个样本到每一类的距离，选取最大的那个作为预测类别（因为在决策边界两侧），decision_function的ovr和ovo列数有差别</span></span><br><span class="line"><span class="comment"># predict就是正常的返回类别对应的数字</span></span><br><span class="line"><span class="comment"># svm的score返回的是精度类似于metrics.accuracy_score(x_train, y_train)</span></span><br><span class="line"></span><br><span class="line">weight = [<span class="number">2</span>,<span class="number">30</span>,<span class="number">2</span>,<span class="number">30</span>]</span><br><span class="line">clfs = [svm.SVC(C=<span class="number">1</span>, kernel=<span class="string">'linear'</span>, class_weight=&#123;<span class="number">-1</span>:<span class="number">1</span>, <span class="number">1</span>:weight[<span class="number">0</span>]&#125;),</span><br><span class="line">       svm.SVC(C=<span class="number">1</span>, kernel=<span class="string">'linear'</span>, class_weight=&#123;<span class="number">-1</span>:<span class="number">1</span>, <span class="number">1</span>:weight[<span class="number">1</span>]&#125;),</span><br><span class="line">       svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">'rbf'</span>, gamma=<span class="number">0.5</span>, class_weight=&#123;<span class="number">-1</span>:<span class="number">1</span>, <span class="number">1</span>:weight[<span class="number">2</span>]&#125;),</span><br><span class="line">       svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">'rbf'</span>, gamma=<span class="number">0.5</span>, class_weight=&#123;<span class="number">-1</span>:<span class="number">1</span>, <span class="number">1</span>:weight[<span class="number">3</span>]&#125;)]</span><br><span class="line"><span class="comment"># 对于不平衡数据进行权重的分配，保证结果的准确性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用SVM进行回归分析</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">print(<span class="string">'SVR - RBF'</span>)</span><br><span class="line">svr_rbf = svm.SVR(kernel=<span class="string">'rbf'</span>, gamma=<span class="number">0.2</span>, C=<span class="number">100</span>)</span><br><span class="line">svr_rbf.fit(x, y)</span><br><span class="line">print(<span class="string">'SVR - Linear'</span>)</span><br><span class="line">svr_linear = svm.SVR(kernel=<span class="string">'linear'</span>, C=<span class="number">100</span>)</span><br><span class="line">svr_linear.fit(x, y)</span><br><span class="line">print(<span class="string">'SVR - Polynomial'</span>)</span><br><span class="line">svr_poly = svm.SVR(kernel=<span class="string">'poly'</span>, degree=<span class="number">3</span>, C=<span class="number">100</span>)</span><br><span class="line">svr_poly.fit(x, y)</span><br><span class="line">print(<span class="string">'Fit OK.'</span>)</span><br><span class="line"><span class="comment"># 注意这里kernel可以选择linear/rbf分别对应线性核以及高斯核，其中线性核和高斯核分别有两个比较重要的参数</span></span><br><span class="line"><span class="comment"># C和gamma，其中C上面已经解释过了，gamma是一个跟核函数相关的参数，gamma是∂2的倒数，表征高斯核的方差大小，</span></span><br><span class="line"><span class="comment"># 所以∂2的倒数表征的是高斯核的精度，gamma值越大也就数据在训练集上准确度越高，增大过拟合的风险</span></span><br><span class="line"><span class="comment"># PS:kernel还可以选择poly多项式核，配合的参数是degree</span></span><br></pre></td></tr></table></figure></p><h2 id="构建无监督模型"><a href="#构建无监督模型" class="headerlink" title="构建无监督模型"></a>构建无监督模型</h2><p>1.Kmeans聚类<a href="https://www.cnblogs.com/pinard/p/6169370.html" target="_blank" rel="noopener">参考</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets <span class="keyword">as</span> ds</span><br><span class="line">x, y = ds.make_blobs(<span class="number">400</span>, n_features=<span class="number">2</span>, centers=<span class="number">4</span>, cluster_std=(<span class="number">1</span>, <span class="number">2.5</span>, <span class="number">0.5</span>, <span class="number">2</span>), random_state=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 这个函数的功能是根据高斯分布进行构建随机数据，用于后续的聚类分析</span></span><br><span class="line"><span class="comment"># 其中400是产生的样本的个数，n_features是产生的数据的维度，centers是对应数据的类别，cluster_std是定义不同类别的方差</span></span><br><span class="line"><span class="comment"># 这里centers可以是一组array数据，作为中心</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">model = KMeans(n_clusters=<span class="number">4</span>, init=<span class="string">'k-means++'</span>, n_init=<span class="number">10</span>, max_iter=<span class="number">300</span>)</span><br><span class="line">model.fit(x, y)</span><br><span class="line">y_hat = model.predict(x)</span><br><span class="line">model.cluster_centers_ <span class="comment"># 输出对应的聚类中心</span></span><br><span class="line"><span class="comment"># KMeans算法主要有这几个主要参数，n_cluster是聚类类别，init是中心点初始化方法，因为KMeans初值敏感所以需要迭代多次，</span></span><br><span class="line"><span class="comment"># n_init就是控制这个的参数，当数据是非凸集的时候max_iter的设置防止算法不收敛</span></span><br></pre></td></tr></table></figure></p><p>2.AP聚类算法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AffinityPropagation</span><br><span class="line">model = AffinityPropagation(affinity=<span class="string">'euclidean'</span>)</span><br><span class="line">model.fit(x, y)</span><br><span class="line">model.cluster_centers_indices_ <span class="comment"># 返回的是中心点的索引</span></span><br><span class="line">y_hat = model.labels_ <span class="comment">#返回的是预测的数据类别</span></span><br></pre></td></tr></table></figure></p><p>3.DBSCAN算法(<a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">参考1</a>,<a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py" target="_blank" rel="noopener">参考2</a>)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line">model = DBSCAN(eps=m, min_samples=n) <span class="comment"># eps对应算法的半径, min_samples对应算法在ε半径之内包含的最小点数，用来确定核心对象</span></span><br><span class="line">model.fit(x)</span><br><span class="line">y_hat = model.labels_ <span class="comment"># 预测类别值</span></span><br><span class="line">y_coresample = model.core_sample_indices_ <span class="comment"># 预测类别中心点</span></span><br><span class="line">n_clusters_ = len(set(y_hat)) - (<span class="number">1</span> <span class="keyword">if</span> <span class="number">-1</span> <span class="keyword">in</span> y_hat <span class="keyword">else</span> <span class="number">0</span>) <span class="comment"># 注意算法会把噪音点归为-1</span></span><br><span class="line"><span class="comment"># DBSCAN对数据本身没有分布要求，不像Kmeans假设数据是高斯分布的，DBSCAN产生的聚类形状可以不为类圆形</span></span><br><span class="line"><span class="comment"># 固定ε，min_samples越大越严格；固定min_samples，ε越小越严格</span></span><br><span class="line"><span class="comment"># DBSCAN通过计算每个数据点的高局部密度点距离和本身的密度确定每个点的类别，高局部密度点距离大密度大的点为聚类中心</span></span><br><span class="line"><span class="comment"># 高局部密度点距离大密度小的点为噪声，高局部密度点距离小密度大的点为普通点，高局部密度点距离小密度大的点不好判断</span></span><br><span class="line"><span class="comment"># ps:密度就是每个点ε邻域内的点数，高局部密度点距离就是比该点密度大的点中与之距离最小的距离</span></span><br></pre></td></tr></table></figure></p><p>4.MeanShift聚类<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MeanShift</span><br><span class="line">model = MeanShift(bin_seeding=<span class="keyword">True</span>, bandwidth=band_width)</span><br><span class="line"><span class="comment"># 只需要给出圆的半径就好，圆的中心根据圆圈内包含的点的中心不断更新</span></span><br></pre></td></tr></table></figure></p><h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><p>1.模型准确率<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">print</span> <span class="string">'训练集准确率: %.5f'</span>%metrics.accuracy_score(y_train, y_train_hat) <span class="comment"># 先放真实值再放预测值，类型是ndarray</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'测试集准确率: %.5f'</span>%metrics.accuracy_score(y_test, y_test_hat) <span class="comment"># 先放真实值再放预测值，类型是ndarray</span></span><br><span class="line"><span class="comment"># 这里y_train和y_train_hat都是由0、1组成的</span></span><br></pre></td></tr></table></figure></p><p>2.模型的ROC、AUC<a href="https://blog.csdn.net/YE1215172385/article/details/79443552" target="_blank" rel="noopener">micro/macro</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">micro_AUC = metrics.roc_auc_score(y_true, y_probability, average=<span class="string">'micro'</span>)</span><br><span class="line">macro_AUC = metrics.roc_auc_score(y_true, y_probability, average=<span class="string">'macro'</span>)</span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_true.ravel(), y_probability.ravel())</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>), dpi=<span class="number">80</span>, facecolor=<span class="string">'w'</span>)</span><br><span class="line">plt(fpr, tpr, <span class="string">'r-'</span>, lw=<span class="number">2</span>, label=<span class="string">'AUC: %.5f'</span>%micro_AUC)</span><br><span class="line">plt.xlabel(<span class="string">'FPR'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'TPR'</span>)</span><br><span class="line">plt.xticks(np.arange(<span class="number">-0.1</span>, <span class="number">1.1</span>, <span class="number">0.1</span>))</span><br><span class="line">plt.yticks(np.arange(<span class="number">-0.1</span>, <span class="number">1.1</span>, <span class="number">0.1</span>))</span><br><span class="line">plt.title(<span class="string">'ROC curve'</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.grid(b=<span class="keyword">True</span>, ls=<span class="string">':'</span>)</span><br><span class="line">plt.legend(loc = <span class="string">'lower right'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对上述评估标准进行说明，首先y_true和y_probability是一个n*m的ndarray对象，n是样本数，m是响应变量的类别数</span></span><br><span class="line"><span class="comment"># 其中y_true是对应多类别的真实值，每一行只有一个1其余为0，y_probability每一行是不同类别的预测概率</span></span><br><span class="line"><span class="comment"># 在计算AUC得分的时候，如果是多分类问题，有两种计算方式，第一种对每一列绘制ROC计算AUC，即对每一类别分别计算</span></span><br><span class="line"><span class="comment"># 另一种是将n*m的矩阵按行首尾连接，形成一个n*m长的array，对其绘制ROC同时计算AUC，这两种分别是macro和micro</span></span><br><span class="line"><span class="comment"># metrics.roc_curve是用来获得在不同theshold下得到的FPR和TPR的对应值，其中.ravel()方法用来展开数据，相当于按照micro的方法绘制</span></span><br></pre></td></tr></table></figure></p><p>3.模型的MSE(Mean square error)<a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10814010" target="_blank" rel="noopener">参考</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算模型的均方误差</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">metrics.mean_squared_error(y_True, y_hat)</span><br></pre></td></tr></table></figure></p><p>4.模型的其他评估(Mean square error) <a href="https://blog.argcv.com/articles/1036.c" target="_blank" rel="noopener">参考</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score, fbeta_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support, classification_report</span><br><span class="line">fbeta = fbeta_score(y_true, y_hat, beta=beta)</span><br><span class="line">precision = precision_score(y_true, y_hat)</span><br><span class="line">recall = recall_score(y_true, y_hat)</span><br><span class="line">print(<span class="string">'f1 score: \t'</span>, f1_score(y_true, y_hat))</span><br><span class="line">fbeta = fbeta_score(y_true, y_hat, beta=beta)</span><br><span class="line"><span class="comment"># precision score = tp/(tp+fp)和准确度的区别在于只关注正类，准确率是所有类别都关注</span></span><br><span class="line"><span class="comment"># recal score = tp/(tp+fn)</span></span><br><span class="line"><span class="comment"># F1 = 2*(precision*recall)/(precision+recall) F1相当于P和R调和均值，越大模型效果越好</span></span><br><span class="line"><span class="comment"># fbeta_score是P和R的调和均值，beta&lt;1 precision的权重更大，beta&gt;1 recall的权重更大</span></span><br></pre></td></tr></table></figure></p><p>5.聚类方法的评估(<a href="https://blog.csdn.net/Mr_tyting/article/details/76719062" target="_blank" rel="noopener">方法解释</a><a href="https://blog.csdn.net/sinat_26917383/article/details/70577710" target="_blank" rel="noopener">参考1</a>, <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/clustering.html" target="_blank" rel="noopener">参考2</a>)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Homogeneity：'</span>, homogeneity_score(y, y_pred))</span><br><span class="line">print(<span class="string">'completeness：'</span>, completeness_score(y, y_pred))</span><br><span class="line">print(<span class="string">'V measure：'</span>, v_measure_score(y, y_pred))</span><br><span class="line">print(<span class="string">'AMI：'</span>, adjusted_mutual_info_score(y, y_pred))</span><br><span class="line">print(<span class="string">'ARI：'</span>, adjusted_rand_score(y, y_pred))</span><br><span class="line">print(<span class="string">'Silhouette：'</span>, silhouette_score(x, y_pred), <span class="string">'\n'</span>) <span class="comment"># 轮廓系数，1-类内距离/最小类外距离，1的时候最优，真实值也不一定是1..</span></span><br><span class="line"><span class="comment"># 聚类算法在真实的应用中一般难以获取真实的数据标签，除非手动区分，所以上述评估标准比较鸡肋，不深入解释</span></span><br></pre></td></tr></table></figure></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>1.欧氏距离计算<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> euclidean_distances</span><br><span class="line">m = euclidean_distances(data, squared=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 返回一个n*n的矩阵计算的是亮亮之间的距离</span></span><br></pre></td></tr></table></figure></p><h2 id="Pipeline构建"><a href="#Pipeline构建" class="headerlink" title="Pipeline构建"></a>Pipeline构建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pipeline的使用是为了方便对数据处理的流程化，举个简单的example:特征变换+逻辑回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegressionCV</span><br><span class="line">model = Pipeline([</span><br><span class="line">(<span class="string">'poly'</span>, PolynomialFeatures(degree = <span class="number">2</span>, include_bias = <span class="keyword">True</span>)),</span><br><span class="line">(<span class="string">'LR'</span>, LogisticRegressionCV(Cs = np.logspace(<span class="number">-3</span>, <span class="number">4</span>, <span class="number">8</span>), cv = <span class="number">5</span>, fit_intercept = <span class="keyword">False</span>))</span><br><span class="line">])</span><br><span class="line">model.fit(x_train, y_train)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'最优参数:  '</span>, model.get_params(<span class="string">'LR'</span>)[<span class="string">'LR'</span>].C_</span><br><span class="line"><span class="comment"># model包括两步，第一步对特征进行转换，第二步对转换好的特征进行逻辑回归模型构建</span></span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/lanchunhui/article/details/50521648" target="_blank" rel="noopener">Pipeline原理解析</a></p><h2 id="参数验证"><a href="#参数验证" class="headerlink" title="参数验证"></a>参数验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> best_estimator_, RandomizedSearchCV    <span class="comment"># 0.17 grid_search</span></span><br><span class="line">model = svm.SVR(kernel=<span class="string">'rbf'</span>)</span><br><span class="line">c_can = np.logspace(<span class="number">-2</span>, <span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line">gamma_can = np.logspace(<span class="number">-2</span>, <span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line">svr = GridSearchCV(model, param_grid=&#123;<span class="string">'C'</span>: c_can, <span class="string">'gamma'</span>: gamma_can&#125;, cv=<span class="number">5</span>)</span><br><span class="line">svr.fit(x, y)</span><br><span class="line">print(<span class="string">'验证参数：\n'</span>, svr.best_params_)</span><br><span class="line"><span class="comment"># cross validation交叉验证对应的参数</span></span><br><span class="line"><span class="comment"># best_params_对应model的参数，对于内部的参数通过best_params_之后再提取，比如svr中的support_</span></span><br></pre></td></tr></table></figure><h2 id="分类数据进行绘图"><a href="#分类数据进行绘图" class="headerlink" title="分类数据进行绘图"></a>分类数据进行绘图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.对于多分类数据进行划分区域，对坐标轴中的各个点进行预测，并对预测的点给与相应的颜色，例如对x1，x2进行绘图</span></span><br><span class="line">x1_min, x1_max = x1.min(), x1.max()</span><br><span class="line">x2_min, x2_max = x2.min(), x2.max()</span><br><span class="line">x1_array = np.arange(x1_min, x1_max, <span class="number">0.02</span>)</span><br><span class="line">x2_array = np.arange(x2_min, x2_max, <span class="number">0.02</span>)</span><br><span class="line">x1_tmp, x2_tmp = np.meshgrid(x1_array, x2_array)</span><br><span class="line">x_plot = np.stack((x1_tmp.flat, x2_tmp.flat), axis=<span class="number">1</span>) <span class="comment">#x_plot是一个列为2的数据，第一列是x轴值，第二列是y轴值</span></span><br><span class="line">y_meshgrid_hat = model.predict(x_plot)</span><br><span class="line">plt.scatter(x_plot[:,<span class="number">0</span>], x_plot[:,<span class="number">1</span>], c=y_meshgrid_hat, cmap = mpl.colors.ListedColormap([<span class="string">'#77E0A0'</span>, <span class="string">'#FF8080'</span>, <span class="string">'#A0A0FF'</span>]))</span><br><span class="line">plt.scatter(data.iloc[:,<span class="number">2</span>].values, data.iloc[:,<span class="number">3</span>].values, c=pd.Categorical(data.iloc[:,<span class="number">-1</span>]).codes, cmap = mpl.colors.ListedColormap([<span class="string">'g'</span>, <span class="string">'r'</span>, <span class="string">'b'</span>]))</span><br><span class="line"><span class="comment"># cmap参数是对应到不同的y_meshgrid_hat的值的颜色</span></span><br><span class="line">参考: http://sklearn.apachecn.org/cn/<span class="number">0.19</span><span class="number">.0</span>/auto_examples/linear_model/plot_iris_logistic.html<span class="comment">#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 类似的绘图方法</span></span><br><span class="line">x1, x2 = np.mgrid[x1_min:x1_max:<span class="number">500j</span>, x2_min:x2_max:<span class="number">500j</span>]</span><br><span class="line">cm_light = mpl.colors.ListedColormap([<span class="string">'#FF8080'</span>, <span class="string">'#80FF80'</span>, <span class="string">'#8080FF'</span>, <span class="string">'#F0F080'</span>])</span><br><span class="line">cm_dark = mpl.colors.ListedColormap([<span class="string">'r'</span>, <span class="string">'g'</span>, <span class="string">'b'</span>, <span class="string">'y'</span>])</span><br><span class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]</span><br><span class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="keyword">False</span></span><br><span class="line">plt.figure(facecolor=<span class="string">'w'</span>)</span><br><span class="line">plt.pcolormesh(x1, x2, y_test, cmap=cm_light)</span><br><span class="line">plt.contour(x1, x2, y_test, levels=(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>), colors=<span class="string">'k'</span>, linestyles=<span class="string">'--'</span>)</span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], s=<span class="number">20</span>, c=y, cmap=cm_dark, edgecolors=<span class="string">'k'</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line">plt.xlabel(<span class="string">'$X_1$'</span>, fontsize=<span class="number">11</span>)</span><br><span class="line">plt.ylabel(<span class="string">'$X_2$'</span>, fontsize=<span class="number">11</span>)</span><br><span class="line">plt.xlim((x1_min, x1_max))</span><br><span class="line">plt.ylim((x2_min, x2_max))</span><br><span class="line">plt.grid(b=<span class="keyword">True</span>)</span><br><span class="line">plt.tight_layout(pad=<span class="number">2.5</span>)</span><br><span class="line">plt.title(<span class="string">'SVM多分类方法：One/One or One/Other'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.单独做legend，不推荐，需要自己去看对应的类别，如果需要绘图上的调整还是用python吧</span></span><br><span class="line">patchs = [mpatches.Patch(color=<span class="string">'red'</span>, label=<span class="string">'Iris-setosa'</span>),</span><br><span class="line">          mpatches.Patch(color=<span class="string">'green'</span>, label=<span class="string">'Iris-versicolor'</span>),</span><br><span class="line">          mpatches.Patch(color=<span class="string">'blue'</span>, label=<span class="string">'Iris-virginica'</span>)]</span><br><span class="line">plt.legend(handles=patchs, fancybox=<span class="keyword">True</span>, framealpha=<span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.对预测值和真实值进行对比</span></span><br><span class="line">y_order = y_test.argsort()</span><br><span class="line">plt.plot(np.arange(len(y_order)), y_test[y_order], <span class="string">'g-'</span>, label=<span class="string">'True'</span>)</span><br><span class="line">plt.plot(np.arange(len(y_order)), y_test_hat[y_order], <span class="string">'r-'</span>, label=<span class="string">'Predicted'</span>)</span><br><span class="line"><span class="comment"># 分别绘制真实值和预测值，看两者的差别</span></span><br></pre></td></tr></table></figure><h2 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">if</span> os.path.exists(<span class="string">'xxx.model'</span>):</span><br><span class="line">model = joblib.load(<span class="string">'xxx.model'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">joblib.dump(model, <span class="string">'xxx.model'</span>)</span><br></pre></td></tr></table></figure><h2 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h2><ol><li><p>对于使用sklearn过程中出现warning的情况，可以通过使用下列代码进行warning的隐藏</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure></li><li><p>对于使用jupyter notbook的同学可以使用magic命令使得画图的时候直接显示图片代替 “matplotlib.pyplot.show()”</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">% matplotlib inline</span><br></pre></td></tr></table></figure></li></ol><h2 id="还有一些东西没来得及总计，有机会再补充"><a href="#还有一些东西没来得及总计，有机会再补充" class="headerlink" title="还有一些东西没来得及总计，有机会再补充"></a>还有一些东西没来得及总计，有机会再补充</h2><p>#################################################  2018.8.2  ##################################################</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;机器学习算法的实践笔记&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./images/sklearn.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="http://WenchaoXiu.github.io/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>pandas 常用命令笔记</title>
    <link href="http://WenchaoXiu.github.io/2018/07/12/pandas-commands/"/>
    <id>http://WenchaoXiu.github.io/2018/07/12/pandas-commands/</id>
    <published>2018-07-12T14:01:11.000Z</published>
    <updated>2018-07-30T03:31:48.000Z</updated>
    
    <content type="html"><![CDATA[<p class="description">记录一下如何使用pandas</p><p><img src="./images/pandas.jpg" alt="" style="width:100%"></p><a id="more"></a><h1 id="Python包Pandas学习笔记"><a href="#Python包Pandas学习笔记" class="headerlink" title="Python包Pandas学习笔记"></a>Python包Pandas学习笔记</h1><p><strong>最近对pandas做了一个系统的学习，主要参考了这套<a href="https://github.com/guipsamora/pandas_exercises" target="_blank" rel="noopener">习题</a>，方便后续进行python数据分析</strong></p><h2 id="1-数据的基本操作"><a href="#1-数据的基本操作" class="headerlink" title="1.数据的基本操作"></a>1.数据的基本操作</h2><h3 id="1-1-package的载入"><a href="#1-1-package的载入" class="headerlink" title="1.1 package的载入"></a>1.1 package的载入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><h3 id="1-2-数据读取-“-”暂作“或”使用"><a href="#1-2-数据读取-“-”暂作“或”使用" class="headerlink" title="1.2 数据读取(“/”暂作“或”使用)"></a>1.2 数据读取(“/”暂作“或”使用)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># \s+是利用空白符进行分割</span></span><br><span class="line"><span class="comment"># header可以制定列明对应的行数，如果没有就是None</span></span><br><span class="line"><span class="comment"># index_col就是行名对应的列数，如果没有也是None</span></span><br><span class="line"><span class="comment"># names是给定的列名</span></span><br><span class="line">pd.read_csv(path, sep=<span class="string">'\s+'</span>/<span class="string">'\t'</span>/<span class="string">','</span>, header=<span class="number">0</span>/<span class="keyword">None</span>, </span><br><span class="line">index_col=<span class="number">0</span>/<span class="keyword">None</span>, encoding=<span class="string">'utf-8'</span>, names=[...])</span><br></pre></td></tr></table></figure><h3 id="1-3-数据输出"><a href="#1-3-数据输出" class="headerlink" title="1.3 数据输出"></a>1.3 数据输出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># na_rep是填充的缺失值</span></span><br><span class="line"><span class="comment"># header可以给一个list用作数据输出的列名</span></span><br><span class="line"><span class="comment"># columns可以给定数字list用于选取输出特定的列</span></span><br><span class="line">pd.to_csv(path, sep=<span class="string">''</span>, na_rep=<span class="string">'NA'</span>, header=<span class="keyword">True</span>/[<span class="string">'xx'</span>,<span class="string">'xx'</span>], index=<span class="keyword">True</span>, </span><br><span class="line">columns=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3.</span>.], encoding=<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure><h3 id="1-4-创建DataFrame-Series对象"><a href="#1-4-创建DataFrame-Series对象" class="headerlink" title="1.4 创建DataFrame,Series对象"></a>1.4 创建DataFrame,Series对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame和Series都接受list和numpy array数据，若要转换回array可以用df.values或S.values</span></span><br><span class="line"><span class="comment"># DataFrame还可以接受dictionary对象</span></span><br><span class="line">df = pd.DataFrame(np.random.rand(<span class="number">10</span>,<span class="number">5</span>), columns=list(<span class="string">'ABCDE'</span>)) <span class="comment">#列名为ABCDE</span></span><br><span class="line">S = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="1-5-数据类型总结"><a href="#1-5-数据类型总结" class="headerlink" title="1.5 数据类型总结"></a>1.5 数据类型总结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">'string'</span>: list(<span class="string">'abc'</span>),</span><br><span class="line">                   <span class="string">'int64'</span>: list(range(<span class="number">1</span>, <span class="number">4</span>)),</span><br><span class="line">                   <span class="string">'uint8'</span>: np.arange(<span class="number">3</span>, <span class="number">6</span>).astype(<span class="string">'u1'</span>),</span><br><span class="line">                   <span class="string">'float64'</span>: np.arange(<span class="number">4.0</span>, <span class="number">7.0</span>),</span><br><span class="line">                   <span class="string">'bool1'</span>: [<span class="keyword">True</span>, <span class="keyword">False</span>, <span class="keyword">True</span>],</span><br><span class="line">                   <span class="string">'bool2'</span>: [<span class="keyword">False</span>, <span class="keyword">True</span>, <span class="keyword">False</span>],</span><br><span class="line">                   <span class="string">'dates'</span>: pd.date_range(<span class="string">'now'</span>, periods=<span class="number">3</span>).values,</span><br><span class="line">                   <span class="string">'category'</span>: pd.Series(list(<span class="string">"ABC"</span>)).astype(<span class="string">'category'</span>)&#125;)</span><br><span class="line">S.astype(np.int64) <span class="comment"># 数据类型转化</span></span><br><span class="line">S.dtype == np.int64 <span class="comment"># 数据类型判断</span></span><br><span class="line">S.to_frame() <span class="comment"># 将Series数据转换为DataFrame数据</span></span><br><span class="line">pd.to_datetime(df.a) <span class="comment"># 将df数据a列转换为datetime类型</span></span><br></pre></td></tr></table></figure><h3 id="1-6查看数据"><a href="#1-6查看数据" class="headerlink" title="1.6查看数据"></a>1.6查看数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">df.head(<span class="number">10</span>) <span class="comment"># 查看数据前10行</span></span><br><span class="line">df.tail(<span class="number">10</span>) <span class="comment"># 查看数据后10行</span></span><br><span class="line">df.dtypes <span class="comment"># 查看每一列的数据类型</span></span><br><span class="line">df.dtypes[<span class="string">'xxx'</span>] <span class="comment"># 查看xxx列的数据类型</span></span><br><span class="line">df.shape <span class="comment"># 数据对应的行列数</span></span><br><span class="line">df.info() <span class="comment"># 数据索引，类型，内存信息</span></span><br><span class="line">df.describe(include=<span class="string">'all'</span>) <span class="comment"># 对于数字型数据进行分位数统计,all代表对所有数据类型统计</span></span><br><span class="line">S.value_counts(dropna=<span class="keyword">False</span>) <span class="comment"># 对Series里面的值进行个数统计，NA也会统计</span></span><br><span class="line">df.apply(pd.Series.value_counts) <span class="comment">#返回每个值在各列中的个数，没有则用NaN代替</span></span><br><span class="line">df.index <span class="comment"># 返回数据的索引</span></span><br><span class="line">df.columns <span class="comment"># 返回数据的列名</span></span><br><span class="line">df.mean()/min()/median()/std()/count() <span class="comment"># 分别是df的列均值,最小值,中位数,标准差,非空值</span></span><br><span class="line">df.corr() <span class="comment"># 列之间的相关系数</span></span><br><span class="line">df.idxmax(<span class="number">0</span>) <span class="comment"># 每列最大数对应行的index的名</span></span><br><span class="line">df.idxmax(<span class="number">1</span>) <span class="comment"># 每行最大数对应的列名</span></span><br><span class="line">S.is_unique <span class="comment"># 确定Series数据是否是unique的, 返回bool值</span></span><br></pre></td></tr></table></figure><h3 id="1-7数据的截取"><a href="#1-7数据的截取" class="headerlink" title="1.7数据的截取"></a>1.7数据的截取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df.xxx / df[<span class="string">'xxx'</span>] <span class="comment"># 返回数据的某一列，列名为xxx</span></span><br><span class="line">df[[<span class="string">'xx'</span>,<span class="string">'yy'</span>]] <span class="comment"># 选取多列数据</span></span><br><span class="line">df.iloc[<span class="number">3</span>] <span class="comment"># 选取第4行数据</span></span><br><span class="line">df.iloc[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],:] <span class="comment"># 选取多行数据</span></span><br><span class="line">df.loc[[<span class="string">'x'</span>,<span class="string">'y'</span>,<span class="string">'z'</span>],[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]] <span class="comment">#根据行index名和列名进行数据截取</span></span><br><span class="line">df.ix[..,..] <span class="comment"># ix是用行名列名以及行数列数混合赋值的情况下数据的截取</span></span><br><span class="line">df[(df.col&gt;<span class="number">0.5</span>) &amp; (df.col&lt;<span class="number">10</span>)] <span class="comment"># 筛选col大于0.5小于10的行返回</span></span><br><span class="line">df.values <span class="comment"># 返回df对应的numpy array值</span></span><br><span class="line">df.values[<span class="number">10</span>][<span class="number">5</span>] <span class="comment">#求df数据11行6列的值</span></span><br><span class="line">S.str.startswith(<span class="string">'G'</span>) <span class="comment"># Seriers以G开头的字符, 返回bool值</span></span><br><span class="line">S.isin([<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]) <span class="comment"># 返回bool值如果S在list['a','b','c']中, 返回true</span></span><br><span class="line">S != <span class="string">'xxx'</span> <span class="comment"># Series中不为xxx的位置, 返回bool值</span></span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'a'</span>] <span class="comment">#删除数据df的a列</span></span><br><span class="line">df.drop([<span class="string">'B'</span>,<span class="string">'C'</span>],axis=<span class="number">1</span>,inplace=<span class="keyword">True</span>) <span class="comment"># 删除数据df的B,C列, 在原数据上进行修改</span></span><br></pre></td></tr></table></figure><h3 id="1-8数据清洗"><a href="#1-8数据清洗" class="headerlink" title="1.8数据清洗"></a>1.8数据清洗</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">df.columns = [<span class="string">'a'</span>,<span class="string">'b'</span>] <span class="comment"># 更改数据的列名</span></span><br><span class="line">df.isnull().sum() <span class="comment"># 统计各列中缺失值的个数</span></span><br><span class="line">df.notnull().sum() <span class="comment"># 统计各列中非缺失值的个数</span></span><br><span class="line">df.drop_duplicates([...], <span class="string">'first'</span>/<span class="string">'last'</span>/<span class="keyword">False</span>) </span><br><span class="line"><span class="comment"># 移除重复项, list可以是列名也可以是数字序列,first是保留重复中的第一个, last是保留重复中的最后一个, False一个不留, 只要重复都删除</span></span><br><span class="line">df.dropna(axis=<span class="number">0</span>/<span class="number">1</span>, how=<span class="string">'any'</span>/<span class="string">'all'</span>, thresh=n)</span><br><span class="line"><span class="comment"># 移除数据中的缺失行,axis0是行删除,1是列删除</span></span><br><span class="line"><span class="comment"># any是只要有一个就删除,all是所有的都是才删除</span></span><br><span class="line"><span class="comment"># thresh目的是大于n个NA才删除</span></span><br><span class="line">S.fillna(x) <span class="comment"># 对确实值进行填充,填充值为x</span></span><br><span class="line">S.astype(np.float) <span class="comment"># 数据的类型转换</span></span><br><span class="line">S.replace(<span class="number">1</span>, <span class="string">'one'</span>) <span class="comment"># 将1替换为'one'</span></span><br><span class="line">S.replace([<span class="number">1</span>,<span class="number">2</span>], [<span class="string">'one'</span>,<span class="string">'two'</span>]) <span class="comment"># 将1替换为one, 2替换为two</span></span><br><span class="line">df.rename(index/columns=&#123;<span class="string">'old1'</span>:<span class="string">'new1'</span>,<span class="string">'old2'</span>:<span class="string">'new2'</span>&#125;) <span class="comment"># 修改行名列名，将old1改为new1，将old2改为new2</span></span><br><span class="line">df.set_index(<span class="string">'B'</span>) <span class="comment"># 修改index，将B所在的列作为行索引</span></span><br><span class="line">df.sort_index() <span class="comment"># 将数据按照index进行排序</span></span><br><span class="line">df.sort_values([col1, col2], ascending=[<span class="keyword">True</span>,<span class="keyword">False</span>]) <span class="comment"># 根据col1和col2的值进行排序，col1是升序，col2是降序</span></span><br><span class="line">S.argsort() <span class="comment"># 返回Series对应的值的order，S[S.argsort()]返回的是对应的从小到大的Series数值</span></span><br><span class="line">df.reset_index(drop=<span class="keyword">False</span>/<span class="keyword">True</span>, inplace=<span class="keyword">False</span>/<span class="keyword">True</span>) <span class="comment"># 数据的index换成从0开始的, drop是说是否保留原来的index, 保留的话就多一列, inplace是说是否修改原来的df</span></span><br><span class="line">data[<span class="string">'Age'</span>] = pd.cut(data[<span class="string">'Age'</span>], bins=<span class="number">6</span>, labels=np.arange(<span class="number">6</span>)) <span class="comment"># 对数值型数据进行区间分割，分割成6个bin，label用0-5表示</span></span><br><span class="line">np.tile(a, N).flatten() <span class="comment"># 对数据a进行重复</span></span><br></pre></td></tr></table></figure><h3 id="1-9数据分组"><a href="#1-9数据分组" class="headerlink" title="1.9数据分组"></a>1.9数据分组</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df.groupby(col).size() <span class="comment"># 按照col对数据进行分组，并计算每一组的数量, 如果是count的话每列都会计算一次分组后的数量, 比较冗余</span></span><br><span class="line">df.groupby([col1, col2]).mean() <span class="comment"># 按照col1,col2进行分组，并计算各组的均值</span></span><br><span class="line">df.groupby([col1, col2]).agg([<span class="string">'min'</span>, <span class="string">'max'</span>, <span class="string">'mean'</span>]) <span class="comment"># 按照col1col2进行分组，计算各组之间的min, max, mean</span></span><br><span class="line">df.groupby([col1, col2]).agg(&#123;<span class="string">'a'</span>:<span class="string">'min'</span>, <span class="string">'b'</span>:<span class="string">'max'</span>, <span class="string">'c'</span>:<span class="string">'mean'</span>&#125;) <span class="comment"># 按照col1col2进行分组，计算各组之间的a列的min, b列的max, c列的mean</span></span><br><span class="line">df.groupby(col1)[col2].mean() <span class="comment"># 计算按照col1分组的col2对应的均值</span></span><br><span class="line">df.pivot.table(index=col1, values=[col2, col3], aggfun=<span class="string">'mean'</span>) <span class="comment"># 以col1的值为index,以col2,col3值为列进行分组计算各元素平均值</span></span><br><span class="line">df.apply(np.mean) <span class="comment"># 计算每一列的平均值</span></span><br><span class="line">df.apply(np.max, axis=<span class="number">1</span>) <span class="comment">#计算每一行的平均值</span></span><br><span class="line">df.applymap(<span class="keyword">lambda</span> x : x.upper()) <span class="comment"># 对多列数据操作, 这里是对各列都大写</span></span><br><span class="line"><span class="keyword">for</span> name,data <span class="keyword">in</span> df.group(<span class="string">'col'</span>):</span><br><span class="line"><span class="keyword">print</span> name <span class="comment"># col列分类后的值</span></span><br><span class="line"><span class="keyword">print</span> data <span class="comment"># col列名称等于name对应的数据行</span></span><br></pre></td></tr></table></figure><h3 id="1-10-数据合并"><a href="#1-10-数据合并" class="headerlink" title="1.10 数据合并"></a>1.10 数据合并</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pd.concat([df1, df2], axis=<span class="number">1</span>) <span class="comment">#列数相同, 行合在一起</span></span><br><span class="line">pd.concat(frames, keys=[<span class="string">'x'</span>, <span class="string">'y'</span>, <span class="string">'z'</span>], axis=<span class="number">0</span>) <span class="comment">#行数相同, 列合在一起, 每个数据的来源分别标注xyz</span></span><br><span class="line">pd.merge(df1, df2, on=[<span class="string">'key1'</span>,<span class="string">'key2'</span>], how=<span class="string">'outer'</span>/<span class="string">'inner'</span>/<span class="string">'left'</span>/<span class="string">'right'</span>) <span class="comment"># 合并df1和df2,根据df1的key1和df2的key2, 连接方式是外链接..</span></span><br><span class="line">np.vstack((a,b)) <span class="comment"># 乱入一个numpy合并用法，行合并</span></span><br><span class="line"><span class="comment"># pd.join与merge用法类似, 只不过默认是left链接, merge是inner连接</span></span><br></pre></td></tr></table></figure><h2 id="2-数据特殊操作"><a href="#2-数据特殊操作" class="headerlink" title="2.数据特殊操作"></a>2.数据特殊操作</h2><h3 id="2-1-div的用法"><a href="#2-1-div的用法" class="headerlink" title="2.1 div的用法"></a>2.1 div的用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">users = pd.read_table(<span class="string">'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user'</span>, </span><br><span class="line">        sep=<span class="string">'|'</span>, index_col=<span class="string">'user_id'</span>)</span><br><span class="line">total = users.groupby([<span class="string">'occupation'</span>]).gender.count() <span class="comment"># 计算occupation的对应的人数</span></span><br><span class="line">gender = users.groupby([<span class="string">'occupation'</span>,<span class="string">'gender'</span>]).gender.count() <span class="comment"># 计算各职业各性别的人数</span></span><br><span class="line">(gender.div(total, level=<span class="string">'occupation'</span>)*<span class="number">100</span>).unstack() <span class="comment">#计算各职业的各性别的百分比</span></span><br></pre></td></tr></table></figure><h3 id="2-2-时序数据操作"><a href="#2-2-时序数据操作" class="headerlink" title="2.2 时序数据操作"></a>2.2 时序数据操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">df.index = pd.date_range(<span class="string">'2018/1/1'</span>, period=df.shape[<span class="number">0</span>]) <span class="comment"># 添加时间序列作为行名</span></span><br><span class="line">pd.to_datetime(df.a, format=<span class="string">'%Y'</span>) <span class="comment"># 将df的a列转化成datetime类型的年</span></span><br><span class="line">pd.to_datetime(<span class="number">1490195805</span>, unit=<span class="string">'s'</span>) <span class="comment"># 对UNIX时间进行时间转换</span></span><br><span class="line">df[<span class="string">'a'</span>].to_datetime().year/month/day <span class="comment"># df的a列转换为datetime类型之后提取其中的year或者month或者day</span></span><br><span class="line">(df[<span class="string">'a'</span>].to_datetime().max() - df[<span class="string">'a'</span>].to_datetime().min()).days <span class="comment"># 计算a列中最早最晚时间差</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line">dt.datetime(<span class="number">2015</span>,<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># 通过datetime包定义数据时间</span></span><br><span class="line">dt.datetime.today() <span class="comment"># 返回今天对应的时间</span></span><br><span class="line"></span><br><span class="line">df.resample(<span class="string">'10AS'</span>).sum() </span><br><span class="line"><span class="comment"># downsample时序数据, 频率是每10年算各列的加和, S是index从1月1日开始, 不加S则从12月30日开始</span></span><br><span class="line"><span class="comment"># resample的各个字符含义: A-year, M-month, W-week, D-day, H-hour, T-minute, S-second</span></span><br></pre></td></tr></table></figure><p>除了downsample也可以upsample需要插值, 具体使用参考<a href="https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.resample.html" target="_blank" rel="noopener">官方文档</a></p><h2 id="2-绘图命令以各种类型图分类"><a href="#2-绘图命令以各种类型图分类" class="headerlink" title="2.绘图命令以各种类型图分类"></a>2.绘图命令<strong><code>以各种类型图分类</code></strong></h2><h3 id="2-1-barplot"><a href="#2-1-barplot" class="headerlink" title="2.1 barplot"></a>2.1 barplot</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 绘图包的载入</span></span><br><span class="line">% matplotlib inline <span class="comment"># 内嵌画图, 有这个命令就可以省去plt.show()</span></span><br><span class="line">Series.plot(kind = <span class="string">'bar'</span>) <span class="comment"># 绘制条形图</span></span><br><span class="line">plt.xlabel(<span class="string">'xxx'</span>) <span class="comment"># 加x轴label</span></span><br><span class="line">plt.ylabel(<span class="string">'yyy'</span>) <span class="comment"># 加y轴label</span></span><br><span class="line">plt.title(<span class="string">'zzz'</span>) <span class="comment"># 加标题</span></span><br></pre></td></tr></table></figure><p><img src="/images/pandas_commands/p1.png" alt="p1"></p><h3 id="2-2-scatter-plot"><a href="#2-2-scatter-plot" class="headerlink" title="2.2 scatter plot"></a>2.2 scatter plot</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x, y, s=size, c=<span class="string">'green'</span>) <span class="comment"># 绘制散点图, s表明点大小, c表示点的颜色, 这两个参数都可以是list</span></span><br></pre></td></tr></table></figure><p><img src="/images/pandas_commands/p2.png" alt="p2"></p><h3 id="2-3-pie-chart"><a href="#2-3-pie-chart" class="headerlink" title="2.3 pie chart"></a>2.3 pie chart</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.pie([...], labels=[...], colors=[...], explode=(...), startangle=<span class="number">90</span>) <span class="comment"># 饼图的参数都是list, explode参数是为了让饼图不同类之间有空隙的参数</span></span><br></pre></td></tr></table></figure><p><img src="/images/pandas_commands/p3.png" alt="p3"></p><h3 id="2-4-分组scatter-plot"><a href="#2-4-分组scatter-plot" class="headerlink" title="2.4 分组scatter plot"></a>2.4 分组scatter plot</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.lmplot(x=<span class="string">'age'</span>, y=<span class="string">'Fare'</span>, data=df, hue=<span class="string">'sex'</span>, fit_reg=<span class="keyword">False</span>) <span class="comment"># 绘制以age和Fare为xy轴的散点图, 以性别分类</span></span><br></pre></td></tr></table></figure><p><img src="/images/pandas_commands/p4.png" alt="p4"></p><h3 id="2-5-histgram"><a href="#2-5-histgram" class="headerlink" title="2.5 histgram"></a>2.5 histgram</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(S, bins=np.arange(<span class="number">0</span>, <span class="number">600</span>, <span class="number">10</span>)) <span class="comment"># 绘制直方图, bin可以自己定义</span></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = sns.dstplot(S) <span class="comment"># 绘制直方图, 带一条正态拟合曲线</span></span><br><span class="line">a.set(xlabel=<span class="string">''</span>, ylabel=<span class="string">''</span>, title=<span class="string">''</span>) <span class="comment"># 设定label</span></span><br><span class="line">sns.despline() <span class="comment"># 去掉右侧上侧的边框</span></span><br></pre></td></tr></table></figure><p><img src="/images/pandas_commands/p5.png" alt="p5"></p><h3 id="2-6-correlation-plot"><a href="#2-6-correlation-plot" class="headerlink" title="2.6 correlation plot"></a>2.6 correlation plot</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.jointplot(x=<span class="string">''</span>, y=<span class="string">''</span>, data=df) <span class="comment">#绘制带correlation散点图+histgram</span></span><br></pre></td></tr></table></figure><p><img src="/images/pandas_commands/p6.png" alt="p6"></p><h3 id="2-7-pairwise散点图"><a href="#2-7-pairwise散点图" class="headerlink" title="2.7 pairwise散点图"></a>2.7 pairwise散点图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.pairplot(df) <span class="comment"># 多列数据绘制散点图</span></span><br></pre></td></tr></table></figure><p><img src="/images/pandas_commands/p7.png" alt="p7"></p><h3 id="2-8-分类boxplot图"><a href="#2-8-分类boxplot图" class="headerlink" title="2.8 分类boxplot图"></a>2.8 分类boxplot图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.boxplot(x=<span class="string">''</span>, y=<span class="string">''</span>, data=df, hue=<span class="string">''</span>) <span class="comment"># 分类box plot, hue是类别</span></span><br></pre></td></tr></table></figure><p><img src="/images/pandas_commands/p8.png" alt="p8"></p><h3 id="2-9-分类boxplot点图"><a href="#2-9-分类boxplot点图" class="headerlink" title="2.9 分类boxplot点图"></a>2.9 分类boxplot点图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.stripplot(x=<span class="string">''</span>, y=<span class="string">''</span>, data=df, hue=<span class="string">'sex'</span>, jitter=<span class="keyword">True</span>) </span><br><span class="line"><span class="comment"># 绘制类似于boxplot的图，只不过画的是每个box里面的点, x轴是不同数据类</span></span><br></pre></td></tr></table></figure><p><img src="/images/pandas_commands/p9.png" alt="p9"></p><hr>]]></content>
    
    <summary type="html">
    
      Python包Pandas学习笔记
    
    </summary>
    
      <category term="python学习" scheme="http://WenchaoXiu.github.io/categories/python%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>hexo command</title>
    <link href="http://WenchaoXiu.github.io/2018/07/12/firstblog_hexo/"/>
    <id>http://WenchaoXiu.github.io/2018/07/12/firstblog_hexo/</id>
    <published>2018-07-12T08:07:52.000Z</published>
    <updated>2018-07-13T06:17:09.000Z</updated>
    
    <content type="html"><![CDATA[<p class="description">记录一下如何使用hexo进行博客撰写</p><a id="more"></a><h2 id="本地调试"><a href="#本地调试" class="headerlink" title="本地调试"></a>本地调试</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s --debug -p 5000</span><br></pre></td></tr></table></figure><h2 id="网上更新"><a href="#网上更新" class="headerlink" title="网上更新"></a>网上更新</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><h2 id="新建md文件"><a href="#新建md文件" class="headerlink" title="新建md文件"></a>新建md文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new post md_name</span><br></pre></td></tr></table></figure><p>更多<a href="http://moxfive.xyz/2015/12/21/common-hexo-commands/" target="_blank" rel="noopener">hexo命令</a><br>更多内容参见<a href="https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html" target="_blank" rel="noopener">blog</a><br>对于markdown的使用可以参考这篇<a href="https://www.ofind.cn/blog/HEXO/HEXO%E4%B8%8B%E7%9A%84Markdown%E8%AF%AD%E6%B3%95%28GFM%29%E5%86%99%E5%8D%9A%E5%AE%A2.html" target="_blank" rel="noopener">blog</a></p><hr>]]></content>
    
    <summary type="html">
    
      hexo command
    
    </summary>
    
    
  </entry>
  
</feed>
