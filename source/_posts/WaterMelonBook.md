---
title: 葫芦书读书笔记
tags: []
date: 2019-05-04 17:49:19
permalink:
categories: Machine learning
description: 对葫芦书中一些算法题进行总结
image:
---
<p class="description"></p>

<!-- more -->

## 1. 为什么需要对数值特征进行归一化
	- 为了方便后续进行梯度下降的时候加速收敛
	- 归一化通常主要分为两种：min-max，Z-Score
	- 需要进行归一化的模型：线性回归，LR，SVM，神经网络
	- 树模型不需要进行归一化，因为叶节点生成不涉及到梯度下降

## 2. 数据预处理时，如何处理类别特征
	- 序列编码：类别特征有大小区别
	- one-hot编码：特征的每个值作为一列
	- 二进制编码：将类别映射为二进制数字
	- one-hot编码可能会导致维度灾难，过拟合

## 3. 推荐算法中,如何解决高维度组合特征参数多问题？
	- 可以将M * N矩阵分解为M * K和K * N两个矩阵相乘的形式，这样参数从M * N降到K * (M+N)

## 4. 如何找到有效的组合特征？
	- 可以构建决策树，把决策树各特征顺序组合起来

## 5. 文本表示模型有哪些，各自优缺点？
	- 词袋模型，N-gram，TF-IDF，词嵌入
	- 词嵌入就是将没个词映射到低位空间，类似于主题模型中不同的topic

## 6. Word2Vec是什么以及和LDA有什么区别
	- Word2Vec是google开发的一种词向量嵌入的模型，主要分为分为CBOW和skip-gram两种，最后得到的是词向量的dense vector。
	- LDA是一种生成模型，最后可以得到文档与主题，主题与词之间的概率分布。

## 7. 图像分类中，如果训练数据不足如何解决？
	- 增加数据
		- 对图片进行基本的变换，平移/旋转/裁剪/加噪声/颜色变化/亮度变化等等
		- 生成式对抗网络生成图片
	- 增加数据先验知识
	- 简化模型
		- 减少模型复杂度
		- 增加正则化，drop-out
	- 迁移学习

## 8. 评估指标有哪些？
	- 准确率，精确率，召回率,PR,ROC
		- 准确率: (TP+TF)/(P+F)
		- 精确率: TP/P
		- 召回率: TP/T
		- PR曲线就是横坐标为召回率纵坐标为精确率
		- ROC曲线是以假阳性率为横坐标真阳性率为纵坐标的曲线
			- TPR = TP/P
			- FPR = FP/N
		- 绘制ROC根据PN样本数，从(0,0)开始，每遇到一个正样本纵轴加1/P,反之横轴加1/N
	- PR曲线和ROC曲线的差别
		- 当正负样本比例变化的时候ROC影响比较小，PR影响大

## 9. 什么场景下使用余弦相似度而不是欧氏距离？
	- 关注于角度而不是绝对距离的时候使用余弦相似度，比如文本内容相近，但是长度差距大时
	- 欧式距离则更关注绝对差异，当用户活跃度以及登录次数分别为(1,10),(10,100)不能用余弦相似度

## 10. 余弦距离是否是一个严格的距离定义？
	- 余弦距离是 d = 1-cos<A,B>
	- 满足正定型d>0
	- 满足对称性
	- 但是不满足三角不等式(0,1),(1,1),(1,0)就是个反例

## 11. 如何进行线上测试？
	- 将用户分成两部分，不重复的，一部分用新方法，一部分用旧方法

## 12. 为什么进行了充分的离线测试之后还需要进行A/B测试？
	- 离线评估不能完全消除模型模型过拟合的影响
	- 离线评估无法完全还原线上环境，延迟啊，数据缺失啊之类的
	- 离线评估一般是对模型本身的评估，但是对于某些指标的变化没办法直接估计

## 13. 评估模型的方法？以及各自的优缺点？
	- Holdout检验
		- 直接把样本按照一定比例分成训练集和测试集进行建模和评估
	- K折交叉验证	
		- 将数据等分为k份，其中一份作为验证集，其他作为训练集，每次都选择不同的部分
		- 最后将k次评估指标的均值作为最终指标
	- 自助法
		- 适用于样本较少的时候，每次有放回采样，将多次采样中没采到的作为验证集

## 14. 超参数调优有哪些方法？
	- 网格搜索
		- 设置一定的范围以及一定的步长进行搜索，可以一开始设置较大的步长确定范围
		- 之后再缩短步长精确搜索
	- 随机搜索
		- 不再设定范围随机搜索
	- 贝叶斯优化算法
		- 充分利用了之前搜索的信息，具体来说就是设置一个参数的先验分布，随机的确定参数
		- 根据这个分布进行采样，利用采样信息确定新的参数分布，之后再根据后验分布确定最优参数值
		- 不断迭代

## 15. 如何降低过拟合欠拟合风险？
	- 过拟合
		- 降低模型复杂度
		- 增加正则项
		- 获取更多数据
		- 集成学习减少variance
	- 欠拟合
		- 增加模型复杂度
		- 添加新特征
		- 减少正则化系数

## 16. 加入松弛变量的SVM中可以误差为0吗？+++++++++
	- 不一定，因为此时优化函数分为两部分，

## 17. 逻辑回归与线性回归之间的差别
	- 逻辑回归用于分类，线性回归用来做回归问题，逻辑回归是对数几率回归
	- 两者相同的地方在于都可以用最大似然函数进行优化，都可以使用梯度下降进行参数更新

## 18. 决策树有哪些启发性函数？
	- ID3，C4.5，CART

## 19. ID3，C4.5，CART之间的优缺点？
	- ID3基于信息增益，处理分类问题，无法处理缺失值，无法减枝
	- C4.5是在ID3基础上的改进，可以处理缺失值，通过信息增益率进一步限制了分散化的特征，加入了减枝
	- CART基于基尼指数进行特征筛选，可用于分类和回归，回归的话通过改变损失函数实现，可以减枝可以处理缺失值，产生二叉树，减枝可以使用CCP代价复杂度减枝

## 20. 如何对决策树进行减枝？
	- 预减枝
		- 预减枝是在树生成之间进行判断是否生成的操作
		- 基于三个方面，树的深度，当前叶节点的样本数量，每次分裂的准确率的提升阈值
		- 但是有可能会有欠拟合的风险
	- 后减枝
		-这里有多种策略，以代价复杂度减枝进行说明
		-通过比较减枝前后的误差率的增加作为标准进行减枝操作(选择误差率最低的)
		-具体来说就是R(t)-R(Tt)/(L(t)-1),其中R(t)是减枝后节点的错误样本数，R(Tt)是减枝之前的错误样本数，L(t)是当前节点拥有的叶节点数

## 21. PCA如何降维？
	- 需要找到一个正交基向量w，使得数据映射到w之后保证方差最大(因为默认信号的方差比噪声的大)
	- 通过最大化方差可以获得基向量w
	- 具体步骤：对数据进行去均值化，求解协方差矩阵，进行特征值特征向量求解，取前k个最大的特征值对应的特征向量，将原数据映射

## 22. 线性判别分析
	- 是一种有监督学习，通常用于降维，核心思想是数据映射完之后不同类别之间均值差距最大化

## 23. 简述K-means的具体步骤
	- 数据归一化，处理离群点
	- 随机选取k各中心值
	- 对每个点进行类别标记
	- 对各类点均值进行计算重新取做新的类别中心
	- 不断迭代直到中心点位置基本不变

## 24. k-means的优缺点
	- 数据离群点影响较大
	- K值需要进行选择，对于一些特殊形状的分布没办法进行很好的类别区分
		- K值的选取可以使用手肘法或者GapStatics
	- 数据收初始值影响大
		- 针对初始值的选取Kmeans++做了改进，聚类中心互相离的越远越好

## 25. 高斯混合模型核心思想是什么？如何迭代计算？
	- 高斯混合模型是一种生成模型，通过多个高斯分布权重的不同，采样选取某个高斯分布，进一步进行随机采样
	- 获得了相应的数据点。我们需要根据生成的数据点对模型中各个高斯分布的权重进行估计，同时对每个高斯分布
	- 参数进行估计，使得在相应的参数之下可以获得样本数据概率最大
	- 主要通过EM算法进行各个参数的估计，包含两个步骤：
		- E-step，求解隐变量分布。随机初始参数之后通过随机初始参数估计每个数据
		- M-step，求解在该隐变量分布下使得极大似然函数最大的参数。根据隐变量分布重新估计相应均值，再根据均值估计方差

<!-- ;EM算法推导
;gbdt推导
;算法题： 从右边看被遮挡的二叉树，求露出的node
;只说考到的点有 apriori，KM算法，匈牙利算法，模拟退火，贪婪，蚁群对比
;xgb，rf，lr优缺点场景
 -->
<hr />
